{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/margaretmz/GANs-in-Art-and-Design/blob/main/3_dcgan_color_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgNY4Ko8sw7q"
   },
   "source": [
    "# GAN Training Challenges: DCGAN for Color Images\n",
    "\n",
    "\n",
    "This is a DCGAN implemntation with TensorFlow 2/ Keras, trained to generate 64x64 color images that resemble Cartoon dataset.\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4URtt5nRhpq"
   },
   "source": [
    "Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "7cZ4wzZcJz_e"
   },
   "source": [
    ">[GAN Training Challenges: DCGAN for Color Images](#scrollTo=LgNY4Ko8sw7q)\n",
    "\n",
    ">>[Import](#scrollTo=wcrOk6pURp50)\n",
    "\n",
    ">>[Data](#scrollTo=Mhp9hUESy46A)\n",
    "\n",
    ">>>[Visualization](#scrollTo=PFE71AWEDZBp)\n",
    "\n",
    ">>>[Preprocessing](#scrollTo=Eo818wWbDY7S)\n",
    "\n",
    ">>[The Generator](#scrollTo=BY-sz-IFbWr7)\n",
    "\n",
    ">>[The Discriminator](#scrollTo=9qAUg0qNbXpS)\n",
    "\n",
    ">>[DCGAN](#scrollTo=X0IOiEj-QNXd)\n",
    "\n",
    ">>>[Define the DCGAN class](#scrollTo=urgSpXaPQlKk)\n",
    "\n",
    ">>>[Use Keras Callback to monitor training](#scrollTo=QtlnNc4lQw53)\n",
    "\n",
    ">>>[The DCGAN model - putting it together](#scrollTo=FzjTO5ZqRLVY)\n",
    "\n",
    ">>[Compile DCGAN](#scrollTo=9OubidSS_Lrl)\n",
    "\n",
    ">>[Train DCGAN](#scrollTo=1WhnM4ptN2rp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcrOk6pURp50"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11466,
     "status": "ok",
     "timestamp": 1748494245787,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "VJaCNlDDRz6d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywl3XeMzZ_87"
   },
   "source": [
    "Its always a good practice to check the current version of TensorFlow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1747028754519,
     "user": {
      "displayName": "Hari Krishnan Sudheendran 24MSP3047",
      "userId": "02931182925222568948"
     },
     "user_tz": -330
    },
    "id": "4i5sjvhDXJdW",
    "outputId": "a531a935-d9c0-4f05-fbbb-4e057de3a05f"
   },
   "outputs": [],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1748494284860,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "bb1pzx3yCXGB",
    "outputId": "3685afa8-7d7b-4926-85fe-47c667bc4189"
   },
   "outputs": [],
   "source": [
    "!unzip -u \"/content/cartoon.zip\" -d \"/content/cartoon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1748494291055,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "zCgmISMvQTk-"
   },
   "outputs": [],
   "source": [
    "zalando_data_dir = \"/content/cartoon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2606,
     "status": "ok",
     "timestamp": 1748494296897,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "ZnJZW7Yky-VH",
    "outputId": "599bad77-fb02-4abf-e6c4-b0fc90ccee5e"
   },
   "outputs": [],
   "source": [
    "train_images = tf.keras.utils.image_dataset_from_directory(\n",
    "    zalando_data_dir, label_mode=None, image_size=(64, 64), batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8Tw0wqwCFFN"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFE71AWEDZBp"
   },
   "source": [
    "### Visualization\n",
    "\n",
    "I always like to visualize the training data to get an idea of what the images look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1748494299506,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "-yhIga940hGI",
    "outputId": "eeb94be1-7fdc-4779-ad1b-1d04c73b5f23"
   },
   "outputs": [],
   "source": [
    "image_batch = next(iter(train_images))\n",
    "random_index = np.random.choice(image_batch.shape[0])\n",
    "random_image = image_batch[random_index].numpy().astype(\"int32\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(random_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo818wWbDY7S"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1748494305088,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "7mg-9kvy0car"
   },
   "outputs": [],
   "source": [
    "# Normalize the images to [-1, 1] which is the range of the tanh activation\n",
    "train_images = train_images.map(lambda x: (x - 127.5) / 127.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY-sz-IFbWr7"
   },
   "source": [
    "## The Generator\n",
    "\n",
    "Define the generator model architecture with the Keras `Sequential` API.\n",
    "\n",
    "Use `ReLU` except for the last layer which has `tanh` as the activation.\n",
    "\n",
    "The job of the generator is to make images, to do so in DCGAN, we feed random noise as input to the generator and upsample till the desired image size becomes 64x64x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1748494309602,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "F2mrQ1VCMlX2"
   },
   "outputs": [],
   "source": [
    "# latent dimension of the random noise\n",
    "LATENT_DIM = 10\n",
    "# weight initializer for G per DCGAN paper\n",
    "WEIGHT_INIT = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "# number of channels, 1 for gray scale and 3 for color images\n",
    "CHANNELS = 3 # UPDATED from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1748494314236,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "fC2XWqkar7eD"
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    # create a Keras Sequential model\n",
    "    model = Sequential(name='generator')\n",
    "\n",
    "    # prepare for reshape: FC => BN => RN layers, note: input shape defined in the 1st Dense layer\n",
    "    model.add(layers.Dense(8 * 8 * 256, input_dim=LATENT_DIM))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    # layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "    # 1D => 3D: reshape the output of the previous layer\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "\n",
    "    # upsample to 16x16: apply a transposed CONV => BN => RELU\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2),padding=\"same\", kernel_initializer=WEIGHT_INIT))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    model.add((layers.ReLU()))\n",
    "\n",
    "    # upsample to 32x32: apply a transposed CONV => BN => RELU\n",
    "    model.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2),padding=\"same\", kernel_initializer=WEIGHT_INIT))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    model.add((layers.ReLU()))\n",
    "\n",
    "    # upsample to 64x64: apply a transposed CONV => BN => RELU\n",
    "    model.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2),padding=\"same\", kernel_initializer=WEIGHT_INIT))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    model.add((layers.ReLU()))\n",
    "\n",
    "    # final layer: Conv2D with tanh activation\n",
    "    model.add(layers.Conv2D(CHANNELS, (4, 4), padding=\"same\", activation=\"tanh\"))\n",
    "\n",
    "    # return the generator model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 886,
     "status": "ok",
     "timestamp": 1748494319916,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "EEQz2iA2-XUq",
    "outputId": "b816fe9a-b4e1-4680-fc1f-fe7adb04ed1c"
   },
   "outputs": [],
   "source": [
    "# build the generator model\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1748494323152,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "wEDbYmNL-t99",
    "outputId": "fbe33af9-4ed0-456c-9d2b-0234271f0a7e"
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qAUg0qNbXpS"
   },
   "source": [
    "## The Discriminator\n",
    "\n",
    "Define the discriminator model architecture with the Keras `Sequential` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1748494328951,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "5cyu38q4sB9K"
   },
   "outputs": [],
   "source": [
    "def build_discriminator(height, width, depth, alpha=0.2):\n",
    "    # create a Keras Sequential model\n",
    "    model = Sequential(name='discriminator')\n",
    "    input_shape = (height, width, depth)\n",
    "\n",
    "    # 1. first set of CONV => BN => leaky ReLU layers\n",
    "    model.add(layers.Conv2D(64, (4, 4), padding=\"same\", strides=(2, 2),\n",
    "        input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=alpha))\n",
    "\n",
    "    # 2. second set of CONV => BN => leacy ReLU layers\n",
    "    model.add(layers.Conv2D(128, (4, 4), padding=\"same\", strides=(2, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=alpha))\n",
    "\n",
    "    # 3. third set of CONV => BN => leacy ReLU layers\n",
    "    model.add(layers.Conv2D(128, (4, 4), padding=\"same\", strides=(2, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=alpha))\n",
    "\n",
    "    # flatten and apply dropout\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # sigmoid in the last layer outputting a single value for binary classification\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # return the discriminator model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1748494331189,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "db7E0vd9-5S2",
    "outputId": "ec30259b-e39f-44d0-ee34-bb0316b4cd96"
   },
   "outputs": [],
   "source": [
    "# build the discriminator model\n",
    "discriminator = build_discriminator(64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1748494334618,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "synlf4iD_S9Z",
    "outputId": "24f778c9-0b55-4e17-cad1-c5fdd8f34ade"
   },
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0IOiEj-QNXd"
   },
   "source": [
    "## DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urgSpXaPQlKk"
   },
   "source": [
    "### Define the DCGAN class\n",
    "Subclass `keras.Model` and override `train_step()` to implement the DCGAN architecture. This is where the magic happens and the heart of the DCGAN implementation. [TODO: explain minimax objective function in tutorial, along with visual illustrations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1748494342981,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "Uc4nZaU999iq"
   },
   "outputs": [],
   "source": [
    "class DCGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(DCGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Step 1. Train the discriminator with both real images (label as 1) and fake images (classified as label as 0)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute discriminator loss on real images\n",
    "            pred_real = self.discriminator(real_images, training=True)\n",
    "            real_labels = tf.ones((batch_size, 1))\n",
    "            # UPDATED: apply one-sided label smoothing to real labels\n",
    "            real_labels += 0.05 * tf.random.uniform(tf.shape(real_labels))\n",
    "            d_loss_real = self.loss_fn(real_labels, pred_real)\n",
    "\n",
    "            # Compute discriminator loss on fake images\n",
    "            fake_images = self.generator(noise)\n",
    "            pred_fake = self.discriminator(fake_images, training=True)\n",
    "            fake_labels = tf.zeros((batch_size, 1))\n",
    "            # UPDATED: add random noise to fake labels - not needed\n",
    "            # fake_labels += 0.05 * tf.random.uniform(tf.shape(fake_labels))\n",
    "            d_loss_fake = self.loss_fn(fake_labels, pred_fake)\n",
    "\n",
    "            # total discriminator loss\n",
    "            d_loss = (d_loss_real + d_loss_fake)/2\n",
    "        # Compute discriminator gradients\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        # Update discriminator weights\n",
    "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Step 2. Train the generator (do not update weights of the discriminator)\n",
    "        misleading_labels = tf.ones((batch_size, 1)) # G wants D to think the fake images are real (label as 1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(noise, training=True)\n",
    "            pred_fake = self.discriminator(fake_images, training=True)\n",
    "            g_loss = self.loss_fn(misleading_labels, pred_fake)\n",
    "        # Compute generator gradients\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update generator wieghts\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\n",
    "\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "\n",
    "        return {\"d_loss\": self.d_loss_metric.result(), \"g_loss\": self.g_loss_metric.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtlnNc4lQw53"
   },
   "source": [
    "### Use Keras Callback to monitor training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRdNGQbpSIby"
   },
   "source": [
    "The nice thing about overriding `train_step()` of `keras.Model()` is that we can subclass `Callback` to monitor our DCGAN training.\n",
    "\n",
    "I use the seed noise to create images (with the generator model) throughout the training to see the progress of the generator is making. Also saved the generator model at the end of the training which could potentially be used for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748494347231,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "bGhG9Trm-HpP"
   },
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=100):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Create random noise seed for visualization during traing\n",
    "        self.seed = tf.random.normal([16, latent_dim])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        generated_images = self.model.generator(self.seed)\n",
    "        generated_images = (generated_images * 127.5) + 127.5\n",
    "        generated_images.numpy()\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        for i in range(self.num_img):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            img = keras.utils.array_to_img(generated_images[i])\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "        plt.savefig('epoch_{:03d}.png'.format(epoch))\n",
    "        plt.show()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.generator.save('generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzjTO5ZqRLVY"
   },
   "source": [
    "### The DCGAN model - putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1748494349857,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "MxaV8juyRIOs"
   },
   "outputs": [],
   "source": [
    "dcgan = DCGAN(discriminator=discriminator, generator=generator, latent_dim=LATENT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OubidSS_Lrl"
   },
   "source": [
    "## Compile DCGAN\n",
    "\n",
    "Compile the dcgan model, use the `Adam` optimizer, learning rate of 0.0002, and the **Binary Cross Entropy** as recommended by the paper.\n",
    "\n",
    "In later GAN tutorials, you will learn that we can use other loss functions for other GAN variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1748494352766,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "5pdFGNUs_K-e"
   },
   "outputs": [],
   "source": [
    "D_LR = 0.0001 # UPDATED: discriminator learning rate\n",
    "G_LR = 0.0003 # UPDATED: generator learning rate\n",
    "\n",
    "dcgan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=D_LR, beta_1 = 0.5),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=G_LR, beta_1 = 0.5),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WhnM4ptN2rp"
   },
   "source": [
    "## Train DCGAN\n",
    "Now we simply call `model.fit()` to traing the dcgan model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 942363,
     "status": "ok",
     "timestamp": 1748496077487,
     "user": {
      "displayName": "MOHSIN AMIN 24MSP3086",
      "userId": "17126825055409663869"
     },
     "user_tz": -330
    },
    "id": "GbTrrP_QD4Xf",
    "outputId": "6682653e-276a-4423-c47f-eced3a341085"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 550 # number of epochs\n",
    "dcgan.fit(train_images, epochs=NUM_EPOCHS, callbacks=[GANMonitor(num_img=16, latent_dim=LATENT_DIM)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUI9JAyFBzDV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
