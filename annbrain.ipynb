{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11261713,"sourceType":"datasetVersion","datasetId":7038741}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Define dataset path\n","Train_path = \"/kaggle/input/brain-tumour/Training\"  # Change this to your dataset folder\n","Test_path = \"/kaggle/input/brain-tumour/Testing\"\n","# Image preprocessing\n","image_size = (128, 128)\n","batch_size = 32\n","datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n","\n","train_generator = datagen.flow_from_directory(\n","    Train_path,\n","    target_size=image_size,\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='training'\n",")\n","val_generator = datagen.flow_from_directory(\n","    Train_path,\n","    target_size=image_size,\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='validation'\n",")\n","\n","# Build the ANN model\n","model = Sequential([\n","    Flatten(input_shape=(128, 128, 3)),\n","    Dense(512, activation='relu')\n","    Dropout(0.3),\n","    Dense(256, activation='relu')\n","    Dropout(0.3),\n","    Dense(128, activation='relu')\n","    Dense(4, activation='softmax')  # 4 classes\n","])\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Define EarlyStopping\n","early_stop = EarlyStopping(monitor='val_loss', patience=5,\n","                           restore_best_weights=True, verbose=1)\n","# Train the model\n","model.fit(train_generator, epochs=50, validation_data=val_generator,\n","          callbacks=[early_stop])\n","\n","model.summary()\n","# Example for l1 regularizer\n","#Dense(256, activation='relu', kernel_regularizer=l1(0.001)),\n","    #Dropout(0.3))\n","\n","# Compile the model\n","#model.compile(optimizer=Adam(learning_rate=0.001),\n","             # loss='categorical_crossentropy',\n","              #metrics=['accuracy'])\n","\n","# Define EarlyStopping\n","#early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n","\n","# Fit the model using generators\n","'''history = model.fit(\n","    train_generator,\n","    epochs=50,\n","    validation_data=validation_generator,\n","    callbacks=[early_stop]\n",")'''"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:45:13.582502Z","iopub.execute_input":"2025-04-03T09:45:13.582762Z","iopub.status.idle":"2025-04-03T09:47:32.611952Z","shell.execute_reply.started":"2025-04-03T09:45:13.582742Z","shell.execute_reply":"2025-04-03T09:47:32.611065Z"},"id":"M66nkp9LFLNw","outputId":"246302ed-32bd-4693-835f-b89c1866a183"},"outputs":[{"name":"stdout","text":"Found 2297 images belonging to 4 classes.\nFound 78 images belonging to 4 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 214ms/step - accuracy: 0.3345 - loss: 4.2029 - val_accuracy: 0.1667 - val_loss: 1.8088\nEpoch 2/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 67ms/step - accuracy: 0.5551 - loss: 1.0880 - val_accuracy: 0.2564 - val_loss: 2.1763\nEpoch 3/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 66ms/step - accuracy: 0.6172 - loss: 0.8971 - val_accuracy: 0.2308 - val_loss: 2.3042\nEpoch 4/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 64ms/step - accuracy: 0.6163 - loss: 0.8678 - val_accuracy: 0.2692 - val_loss: 2.0698\nEpoch 5/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.6348 - loss: 0.8257 - val_accuracy: 0.2692 - val_loss: 2.6149\nEpoch 6/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - accuracy: 0.6019 - loss: 0.8161 - val_accuracy: 0.3718 - val_loss: 2.7474\nEpoch 7/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.6617 - loss: 0.7238 - val_accuracy: 0.2821 - val_loss: 2.2113\nEpoch 8/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.6100 - loss: 0.7837 - val_accuracy: 0.3205 - val_loss: 2.2188\nEpoch 9/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 66ms/step - accuracy: 0.6361 - loss: 0.7380 - val_accuracy: 0.2436 - val_loss: 2.6764\nEpoch 10/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.6618 - loss: 0.6813 - val_accuracy: 0.2692 - val_loss: 2.4007\nEpoch 11/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - accuracy: 0.6597 - loss: 0.6754 - val_accuracy: 0.2308 - val_loss: 3.0031\nEpoch 12/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 67ms/step - accuracy: 0.6764 - loss: 0.6515 - val_accuracy: 0.2692 - val_loss: 2.4902\nEpoch 13/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 67ms/step - accuracy: 0.6880 - loss: 0.6294 - val_accuracy: 0.2692 - val_loss: 3.0118\nEpoch 14/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 66ms/step - accuracy: 0.6811 - loss: 0.6404 - val_accuracy: 0.2436 - val_loss: 3.7901\nEpoch 15/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.7003 - loss: 0.5896 - val_accuracy: 0.2692 - val_loss: 3.3488\nEpoch 16/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 62ms/step - accuracy: 0.6935 - loss: 0.5996 - val_accuracy: 0.3205 - val_loss: 2.9385\nEpoch 17/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.6661 - loss: 0.6989 - val_accuracy: 0.3205 - val_loss: 2.5842\nEpoch 18/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - accuracy: 0.6351 - loss: 0.7028 - val_accuracy: 0.3077 - val_loss: 3.1610\nEpoch 19/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - accuracy: 0.6662 - loss: 0.6734 - val_accuracy: 0.3462 - val_loss: 3.0177\nEpoch 20/20\n\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 64ms/step - accuracy: 0.6706 - loss: 0.6509 - val_accuracy: 0.2436 - val_loss: 3.0183\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.2273 - loss: 2.9640\nValidation Accuracy: 0.2436\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":[],"metadata":{"trusted":true,"id":"kyP-T8uwFLN0"},"outputs":[],"execution_count":null}]}